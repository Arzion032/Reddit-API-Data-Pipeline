{"timestamp":"2025-10-22T10:23:01.563710Z","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager","filename":"manager.py","lineno":179}
{"timestamp":"2025-10-22T10:23:01.564816Z","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/main_dag.py","logger":"airflow.models.dagbag.DagBag","filename":"dagbag.py","lineno":593}
{"timestamp":"2025-10-22T10:23:02.031269Z","level":"info","event":"connected to reddit!","logger":"task.stdout"}
{"timestamp":"2025-10-22T10:23:03.297227Z","level":"info","event":"post_lists [{'id': '1occxjl', 'title': 'Developing with production data: who and how?', 'selftext': 'Classic story: you should not work directly in prod, but apply the best devops practices, develop data pipelines in a development environment, debug, deploy in pre-prod, test, then deploy in production.\\n\\nWhat about data analysts? data scientists? statisticians? ad-hoc reports?\\n\\nMost data books focus on the data engineering lifecycle, sometimes they talk about the \"Analytics sandbox\", but they rarely address heads-on the personas doing analytics work in production. Modern data platform allow the decoupling of compute and data, enabling workload isolation to allow users read-only access to production data without affecting production workloads. Other teams perform data replication from production to lower environments. There\\'s also the \"blue-green development architecture\", with two systems with production data.\\n\\nHow are you dealing with users requesting production data?', 'score': 23, 'num_comments': 29, 'author': Redditor(name='aburkh'), 'subreddit_subscribers': 404257, 'created_utc': 1761053214.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1occxjl/developing_with_production_data_who_and_how/', 'upvote_ratio': 0.9, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1ocst3a', 'title': 'Need advice choosing between Data engineer vs Sr Data analyst', 'selftext': 'Hey all I could really use some career advice from this community.\\n\\nI was fortunate to land 2 offers in this market, but now I’m struggling to make the right long term decision.\\n\\nI’m finishing my Master’s in Data Science next semester. I interned last summer at a big company and then started working in my first FT data role as a data analyst at a small company (I’m about 6 months in). My goal is to eventually move into Data Science/ML maybe ML engineer and end up in big tech.\\n\\nOption A: Data Engineer I\\n* Industry: Finance. This one pays $15k more. I’ll be working with a smaller team and I’d be the main technical person on the team. So no strong mentorship and I’ll have the pressure to “figure it out” on my own. \\n\\nOption B: Senior Data Analyst\\n* Industry: retail at a large org.\\n\\nI’m nervous about being the only engineer on a team this early in my career…But I’m also worried about not being technical enough as a data analyst and not being technical. \\n\\nWhat would you do in my shoes?\\nGo hard into engineering now and level up fast even if it’s stressful without much support? Or take the analyst role at a big company, build brand and transition later?\\n\\nWould appreciate any advice from people who’ve been on either path.\\n', 'score': 13, 'num_comments': 17, 'author': Redditor(name='Agile_Yak3819'), 'subreddit_subscribers': 404257, 'created_utc': 1761090065.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocst3a/need_advice_choosing_between_data_engineer_vs_sr/', 'upvote_ratio': 0.81, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1ocie74', 'title': 'Our 7 Snowflake query optimization tips and why they work', 'selftext': \"Hope y'all find it useful!\", 'score': 9, 'num_comments': 1, 'author': Redditor(name='hornyforsavings'), 'subreddit_subscribers': 404257, 'created_utc': 1761065889.0, 'url': 'https://blog.greybeam.ai/snowflake-query-optimization/', 'upvote_ratio': 0.91, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1od2x2k', 'title': 'Parquet vs. Open Table Formats: Worth the Metadata Overhead?', 'selftext': 'I recently ran into all sorts of pain working directly with raw Parquet files for an analytics project  broken schemas, partial writes, and painfully slow scans.   \\nThat experience made me realize something simple: Parquet is *just* a storage format. It’s great at compression and column pruning, but that’s where it ends. No ACID guarantees, no safe schema evolution, no time travel, and a whole lot of chaos when multiple jobs touch the same data.\\n\\nThen I explored open table formats like Apache Iceberg, Delta Lake, and Hudi  and it was like adding a missing layer of order on top  impressive is what they are bringing in \\n\\n* ACID transaction**s** through atomic metadata commits\\n* Schema evolution without having to rewrite everything\\n* for easy rollbacks and historical analysis we have Time travel \\n* you can scan millions of files in milliseconds by Manifest indexing another cool thing \\n* not to forget the hidden partitions \\n\\nIn practice, these features made a *huge* difference  reliable BI queries running on the same data as streaming ETL jobs, painless GDPR-style deletes, and background compaction that keeps things tidy.\\n\\nBut it does make you think  is that extra metadata layer really worth the added complexity?  \\n Or can clever workarounds and tooling keep raw Parquet setups running just fine at scale?\\n\\nWrote a blog on this that i am sharing here looking forward to your thoughts ', 'score': 5, 'num_comments': 1, 'author': Redditor(name='DevWithIt'), 'subreddit_subscribers': 404257, 'created_utc': 1761122771.0, 'url': 'https://olake.io/blog/iceberg-vs-parquet-table-format-vs-file-format', 'upvote_ratio': 0.86, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1ocqznw', 'title': 'Tools for automated migration away from Informatica', 'selftext': 'Has anyone ever had any success using tools like DataBricks Lakebridge or Snowflake\\'s SnowConvert to migrate Informatica powercenter ETL pipelines to another platform? I assume at best they \"kind of work sometimes for some things\" but am curious to hear anyone\\'s actual experience with them in the wild.', 'score': 5, 'num_comments': 2, 'author': Redditor(name='BadKafkaPartitioning'), 'subreddit_subscribers': 404257, 'created_utc': 1761085359.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocqznw/tools_for_automated_migration_away_from/', 'upvote_ratio': 1.0, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1oca4c7', 'title': 'Help a noob: CI/CD pipelines with medallion architecture', 'selftext': 'Hello,   \\nI have worked for a few years as an analyst (self taught) and now I am trying to get into data engineering. I am trying to simply understand how to structure a DWH using\\xa0**medallion architecture**\\xa0(Bronze → Silver → Gold) across\\xa0**multiple environments**\\xa0(Dev / Test / Prod).\\n\\nNow, with the last company I worked with, they simply had two databases, staging, and production. Staging is basically the data lake and they transformed all the data to production. I understand this is not best practice. \\n\\nI thought if I wanted to have a proper structure in my DWH, I was thinking of this:\\n\\nDWH |\\n\\n\\\\-> StagingDB -> BronzeSchema, SilverSchema, GoldSchema\\n\\n\\\\-> TestDB -> BronzeSchema, SilverSchema, GoldSchema\\n\\n\\\\-> ProdDB -> BronzeSchema, SilverSchema, GoldSchema\\n\\nWould you even create a bronze layer on staging and test DBs or not really? I mean it is just the raw data no? ', 'score': 7, 'num_comments': 3, 'author': Redditor(name='Nomad_chh'), 'subreddit_subscribers': 404257, 'created_utc': 1761045229.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1oca4c7/help_a_noob_cicd_pipelines_with_medallion/', 'upvote_ratio': 0.82, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1ocz7qu', 'title': 'What is the best way to orchestrate dbt job in aws', 'selftext': 'I recently joined my company, and they currently run dbt jobs using AWS Step Functions and a Fargate task that executes the project, and so on.\\n\\nHowever, I’m not sure if this is the best approach to orchestrate dbt jobs. Another important point is that the company manages most workflows through events following a DDD (Domain-Driven Design) pattern.\\n\\nRight now, there’s a case where a process depends on two different Step Functions before triggering another process.\\nThe challenge is that these Step Functions run at different times and don’t depend on each other.\\nAdditionally, in the future, there might be other processes that depend on those same Step Functions, but not necessarily on this one\\n\\nIn my opinion, Airflow doesn’t fit well here.\\n\\nWhat do you think would be a better way to manage these processes? Would it make sense to build something more custom for these types of cases', 'score': 4, 'num_comments': 3, 'author': Redditor(name='jonathanrodrigr12'), 'subreddit_subscribers': 404257, 'created_utc': 1761108844.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocz7qu/what_is_the_best_way_to_orchestrate_dbt_job_in_aws/', 'upvote_ratio': 0.71, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1ocalca', 'title': 'Any good PR review tools for data stacks?', 'selftext': 'Has anyone tried using PR review tools like **CodeRabbit** or **Greptile** for data engineering workflows (dbt, Airflow, Snowflake, etc.)?\\n\\nIf anyone can share theier experience on if they can handle things like schema changes, query optimization, or data quality checks well, or if they’re more tuned for general code reviews (which I m mostly expecting).', 'score': 3, 'num_comments': 0, 'author': Redditor(name='Hot_Donkey9172'), 'subreddit_subscribers': 404257, 'created_utc': 1761046763.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocalca/any_good_pr_review_tools_for_data_stacks/', 'upvote_ratio': 0.72, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1od0cqr', 'title': 'Anyone experienced with jOOQ as SQL transpiler?', 'selftext': 'Does anyone have experience with jOOQ (https://github.com/jOOQ/jOOQ) as a transpiler between two different SQL dialects? We are searching for options in Java to run queries from other dialects on Exasol without the users having to rewrite them.', 'score': 2, 'num_comments': 0, 'author': Redditor(name='exagolo'), 'subreddit_subscribers': 404257, 'created_utc': 1761112892.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1od0cqr/anyone_experienced_with_jooq_as_sql_transpiler/', 'upvote_ratio': 0.67, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1ocy398', 'title': 'The Death of Thread Per Core', 'selftext': '', 'score': 2, 'num_comments': 0, 'author': Redditor(name='sionescu'), 'subreddit_subscribers': 404257, 'created_utc': 1761105202.0, 'url': 'https://buttondown.com/jaffray/archive/the-death-of-thread-per-core/', 'upvote_ratio': 0.67, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1ocksmo', 'title': 'Thoughts on Using Synthetic Tabular data for DE projects ?', 'selftext': \"Thoughts on using Synthetic Data for Projects ?\\n\\nI'm currently a DB Specialist with 3 YOE learning Spark, DBT, Python, Airflow and AWS to switch to DE roles.\\n\\nI’d love some feedback on a portfolio project I’m working on. It’s basically a modernized spin on the kind of work I do at my job, a Transaction Data Platform with a multi-step ETL pipeline.\\n\\nQuick overview of setup:\\n\\nDB structure:\\n\\nDimensions = Bank -> Account -> Routing\\n\\nFact = Transactions -> Transaction\\\\_Steps\\n\\nHistory = Hist_Transactions -> Hist_Transaction_Steps (identical to fact tables, just one extra column) \\n\\nI mocked up 3 regions -> 3 banks per region -> 3 accounts per bank -> 702 unique directional routings.\\n\\nA Python script first assigns following parameters to each routing:\\n\\ntype (High Intensity/Frequency/Normal)\\n\\ncountry\\\\_code, region, cross\\\\_border\\n\\nbase\\\\_freq, base\\\\_amount, base\\\\_latency, base\\\\_success\\n\\nvolatility vars (freq/amount/latency/success)\\n\\nThen the synthesizer script uses above paramters to spit out 85k-135k records per day, and 5x times Transaction\\\\_Steps \\n\\nAnomaly engine randomly spikes volatility (50–250x) \\\\~5 times a week for a random routing, the aim is (hopefully) the pipeline will detect the anomalies.\\n\\nPipeline workflow:\\n\\nBatch runs daily (simulating off business hours migration).\\n\\nEvery day data older than 1 month in live table is moved to history tables (partitioned by day and OLTP compressed) \\n\\nThen the partitions older than a month in history tables are exported to Parquet (maybe I'll create a Data lake or something) cold storage and stored. \\n\\nThe current day's transactions are transformed through DBT, to generate 12 marts, helping in anomaly detection and system monitoring \\n\\nA Great Expectation + Python layer takes care of data quality and Anomaly detection\\n\\nFinally for visualization and ease of discussion I'm generating a streamlit dashboard from above 12 marts.\\n\\nMain concerns/questions:\\n\\n1. Since this is just inspired by my current work (I didn’t use real table names/logic, just the concept), should I be worried about IP/overlap ?\\n2. I’ve done a barebones version of this in shell+SQL, so I personally know business and technical requirements and possible issues in this project, it feels really straightforward.  Do you think this is a solid enough project to showcase for DE roles at product-based-companies / fintechs (0–3 YOE range)?\\n3. Thoughts on using synthetic data? I’ve tried to make it noisy and realistic, but since I’ll always have control, I feel like I'm missing something critical that only shows up in real-world messy data?\\n\\nWould love any outside perspective\\n\\nThis would ideally be the portfolio project, and there's one more planned using spark where I'm just cleaning and merging Spotify datasets from different types (CSV, json, sqlite, parquet etc) from Kaggle, it's just a practice project to showcase spark understanding. \\n\\n**TLDR:**  \\nBuilt a synthetic transaction pipeline (750k+ txns, 3.75M steps, anomaly injection, DBT marts, cold storage). Looking for feedback on:\\n\\n* IP concerns (inspired by work but no copied code/keywords)\\n* Whether it’s a strong enough DE project for Product Based Companies and Fintech. \\n* Pros/cons of using synthetic vs real-world messy data\", 'score': 2, 'num_comments': 3, 'author': Redditor(name='Markymark285'), 'subreddit_subscribers': 404257, 'created_utc': 1761071216.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocksmo/thoughts_on_using_synthetic_tabular_data_for_de/', 'upvote_ratio': 0.67, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1ocery5', 'title': 'Anyone else use AWS Redshift Zero-ETL in US-EAST-1?', 'selftext': \"This is a service that basically puts a read replica of an RDS streaming into your Redshift data warehouse. \\n\\nWe have this set up in our environment and it runs many critical systems. After the nightmares of yesterday I checked this morning after getting some complaints from unhappy users about stale data and our ZETL integrations appear to have disappeared entirely. I can see the data and it appears to have stopped updating coincident with yesterday's outage. Looks like I'll have to completely remake these. This is pretty irritating because I can't find any information anywhere from AWS about the outage having *deleted* this infrastructure.\", 'score': 4, 'num_comments': 1, 'author': Redditor(name='bingbongbangchang'), 'subreddit_subscribers': 404257, 'created_utc': 1761057629.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocery5/anyone_else_use_aws_redshift_zeroetl_in_useast1/', 'upvote_ratio': 0.75, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1occmld', 'title': 'DBT unit tests on Redshift', 'selftext': \"Hello!\\n\\nI'm trying to implement unit tests for the DBT models used by my team, so we have more trust on those models and their logic. However I'm getting stuck when the model contains a SUPER-typed column for JSON data.\\n\\nIf I write the JSON object inside a string in the YAML file of the test, then DBT expects unquoted JSON. If I remove the quotes around the JSOn object, then I get a syntax error on the YAML file. I also tried writing the JSON object as a YAML object with indent but it fails too. \\n\\nWhat should I do ?\", 'score': 2, 'num_comments': 5, 'author': Redditor(name='Adrien0623'), 'subreddit_subscribers': 404257, 'created_utc': 1761052451.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1occmld/dbt_unit_tests_on_redshift/', 'upvote_ratio': 0.63, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1od2c18', 'title': 'Data Modeling for the Agentic Era: Semantics, Speed, and Stewardship', 'selftext': '', 'score': 1, 'num_comments': 0, 'author': Redditor(name='sspaeti'), 'subreddit_subscribers': 404257, 'created_utc': 1761120465.0, 'url': 'https://www.rilldata.com/blog/data-modeling-for-the-agentic-era-semantics-speed-and-stewardship', 'upvote_ratio': 0.67, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1ocvodq', 'title': 'Data Streaming Delivery Semantics', 'selftext': 'https://preview.redd.it/h4k1298ikkwf1.png?width=1744&format=png&auto=webp&s=ebf666f0ad1f466edc7e8802e4087f65c35dfd4c\\n\\n[https://codepen.io/gangtao/full/raxdOOK](https://codepen.io/gangtao/full/raxdOOK) ', 'score': 1, 'num_comments': 0, 'author': Redditor(name='gangtao'), 'subreddit_subscribers': 404257, 'created_utc': 1761098042.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocvodq/data_streaming_delivery_semantics/', 'upvote_ratio': 0.67, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1oce5db', 'title': 'Schema Evolution in GCP', 'selftext': 'Hi, i have experience with ELT pipelines on GCP. One that loads data from Postgres tables data into BigQuery and another one that loads CSV files data from SFTP server to BigQuery. \\n\\n* Postgres to BQ:\\n\\nSource -> BQ Bronze layer \\nDataflow (custom template for multi table ingestion in single job)\\n\\nBQ Bronze-> BQ Silver Layer -> BQ Gold \\n\\n————————————————————\\n\\n* SFTP to BQ:\\n\\nSFTP -> GCS\\nAirflow (Composer) SFTP to GCS operator\\n\\nGCS -> BQ Bronze\\nGCS to BQ airflow operator\\n\\nBQ Bronze layer -> BQ Silver Layer -> BQ Gold \\n\\n—————————————————————\\n\\nBQ bronze to silver in both cases is handled using a stored procedure that handles upsert of Bronze data (cleaned data) to Silver. Dataform for Silver to Gold layer.\\n\\nNow, could you please explain how below could be handled in both cases?:\\n\\n1. Schema Evolution \\n2. Handling both incremental or Full load as per requirement using same pipeline (if possible)\\n3. Handling Large Volumes of data (TB) \\n4. Frameworks/tools/methods for Data Quality checks and data validations\\n\\n\\nThank you in advance! \\n\\n\\n', 'score': 1, 'num_comments': 0, 'author': Redditor(name='FeeOk6875'), 'subreddit_subscribers': 404257, 'created_utc': 1761056138.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1oce5db/schema_evolution_in_gcp/', 'upvote_ratio': 0.57, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1ocaik3', 'title': 'Integrating sqlmesh models with Dagster', 'selftext': 'Hey folks,\\n\\nI’m currently using Dagster as the orchestrator in my team’s data stack and I’m considering incorporating sqlmesh as our transformation library. But, I can’t really figure out a way to integrate my sqlmesh models with Dagster so that they show up as individual assets. Has anyone had any luck in achieving this ? How did you go about doing it ?', 'score': 1, 'num_comments': 1, 'author': Redditor(name='YameteGPT'), 'subreddit_subscribers': 404257, 'created_utc': 1761046512.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocaik3/integrating_sqlmesh_models_with_dagster/', 'upvote_ratio': 0.57, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1od0cdl', 'title': 'What do you think it would happen to dbt certificates since it was merged with Fivetran?', 'selftext': 'A bit of context, I am coming from non tech education so I am always looking for ways to learn more about data engineering. \\n\\nI was thinking to prepare for the dbt certificates but since the news came out. I am observing. \\n\\nWhat do you think?', 'score': 0, 'num_comments': 2, 'author': Redditor(name='Ragnareuk'), 'subreddit_subscribers': 404257, 'created_utc': 1761112857.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1od0cdl/what_do_you_think_it_would_happen_to_dbt/', 'upvote_ratio': 0.5, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1oco9fs', 'title': 'Date time granularity', 'selftext': 'Hi all,\\n\\nI have some room booking data I need to do some time-related calculations with using Power Bi.\\n\\n1st table has room bookings data with room name, meeting start date time, meeting end date time, snapshot\\\\_date, etc.\\n\\nAs part of my ETL I am already building the snapshot\\\\_date rows based on the meeting start date time and meeting end date time.\\n\\n2nd table has room occupancy data which has room name, start date time, stop date time and usage which are in hour buckets.\\n\\nI have a dim date table connected to snapshot\\\\_date in the room bookings table and start date time in the room occupancy table.\\n\\nQuestion is do I need to have my room bookings data at the same time granularity (hourly) as the room occupancy data to make life easier with time calculations moving forward.\\n\\nCheers', 'score': 0, 'num_comments': 2, 'author': Redditor(name='ImFizzyGoodNice'), 'subreddit_subscribers': 404257, 'created_utc': 1761078928.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1oco9fs/date_time_granularity/', 'upvote_ratio': 0.4, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}]","logger":"task.stdout"}
{"timestamp":"2025-10-22T10:23:03.297893Z","level":"info","event":"[{'id': '1occxjl', 'title': 'Developing with production data: who and how?', 'selftext': 'Classic story: you should not work directly in prod, but apply the best devops practices, develop data pipelines in a development environment, debug, deploy in pre-prod, test, then deploy in production.\\n\\nWhat about data analysts? data scientists? statisticians? ad-hoc reports?\\n\\nMost data books focus on the data engineering lifecycle, sometimes they talk about the \"Analytics sandbox\", but they rarely address heads-on the personas doing analytics work in production. Modern data platform allow the decoupling of compute and data, enabling workload isolation to allow users read-only access to production data without affecting production workloads. Other teams perform data replication from production to lower environments. There\\'s also the \"blue-green development architecture\", with two systems with production data.\\n\\nHow are you dealing with users requesting production data?', 'score': 23, 'num_comments': 29, 'author': Redditor(name='aburkh'), 'subreddit_subscribers': 404257, 'created_utc': 1761053214.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1occxjl/developing_with_production_data_who_and_how/', 'upvote_ratio': 0.9, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1ocst3a', 'title': 'Need advice choosing between Data engineer vs Sr Data analyst', 'selftext': 'Hey all I could really use some career advice from this community.\\n\\nI was fortunate to land 2 offers in this market, but now I’m struggling to make the right long term decision.\\n\\nI’m finishing my Master’s in Data Science next semester. I interned last summer at a big company and then started working in my first FT data role as a data analyst at a small company (I’m about 6 months in). My goal is to eventually move into Data Science/ML maybe ML engineer and end up in big tech.\\n\\nOption A: Data Engineer I\\n* Industry: Finance. This one pays $15k more. I’ll be working with a smaller team and I’d be the main technical person on the team. So no strong mentorship and I’ll have the pressure to “figure it out” on my own. \\n\\nOption B: Senior Data Analyst\\n* Industry: retail at a large org.\\n\\nI’m nervous about being the only engineer on a team this early in my career…But I’m also worried about not being technical enough as a data analyst and not being technical. \\n\\nWhat would you do in my shoes?\\nGo hard into engineering now and level up fast even if it’s stressful without much support? Or take the analyst role at a big company, build brand and transition later?\\n\\nWould appreciate any advice from people who’ve been on either path.\\n', 'score': 13, 'num_comments': 17, 'author': Redditor(name='Agile_Yak3819'), 'subreddit_subscribers': 404257, 'created_utc': 1761090065.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocst3a/need_advice_choosing_between_data_engineer_vs_sr/', 'upvote_ratio': 0.81, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1ocie74', 'title': 'Our 7 Snowflake query optimization tips and why they work', 'selftext': \"Hope y'all find it useful!\", 'score': 9, 'num_comments': 1, 'author': Redditor(name='hornyforsavings'), 'subreddit_subscribers': 404257, 'created_utc': 1761065889.0, 'url': 'https://blog.greybeam.ai/snowflake-query-optimization/', 'upvote_ratio': 0.91, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1od2x2k', 'title': 'Parquet vs. Open Table Formats: Worth the Metadata Overhead?', 'selftext': 'I recently ran into all sorts of pain working directly with raw Parquet files for an analytics project  broken schemas, partial writes, and painfully slow scans.   \\nThat experience made me realize something simple: Parquet is *just* a storage format. It’s great at compression and column pruning, but that’s where it ends. No ACID guarantees, no safe schema evolution, no time travel, and a whole lot of chaos when multiple jobs touch the same data.\\n\\nThen I explored open table formats like Apache Iceberg, Delta Lake, and Hudi  and it was like adding a missing layer of order on top  impressive is what they are bringing in \\n\\n* ACID transaction**s** through atomic metadata commits\\n* Schema evolution without having to rewrite everything\\n* for easy rollbacks and historical analysis we have Time travel \\n* you can scan millions of files in milliseconds by Manifest indexing another cool thing \\n* not to forget the hidden partitions \\n\\nIn practice, these features made a *huge* difference  reliable BI queries running on the same data as streaming ETL jobs, painless GDPR-style deletes, and background compaction that keeps things tidy.\\n\\nBut it does make you think  is that extra metadata layer really worth the added complexity?  \\n Or can clever workarounds and tooling keep raw Parquet setups running just fine at scale?\\n\\nWrote a blog on this that i am sharing here looking forward to your thoughts ', 'score': 5, 'num_comments': 1, 'author': Redditor(name='DevWithIt'), 'subreddit_subscribers': 404257, 'created_utc': 1761122771.0, 'url': 'https://olake.io/blog/iceberg-vs-parquet-table-format-vs-file-format', 'upvote_ratio': 0.86, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1ocqznw', 'title': 'Tools for automated migration away from Informatica', 'selftext': 'Has anyone ever had any success using tools like DataBricks Lakebridge or Snowflake\\'s SnowConvert to migrate Informatica powercenter ETL pipelines to another platform? I assume at best they \"kind of work sometimes for some things\" but am curious to hear anyone\\'s actual experience with them in the wild.', 'score': 5, 'num_comments': 2, 'author': Redditor(name='BadKafkaPartitioning'), 'subreddit_subscribers': 404257, 'created_utc': 1761085359.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocqznw/tools_for_automated_migration_away_from/', 'upvote_ratio': 1.0, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1oca4c7', 'title': 'Help a noob: CI/CD pipelines with medallion architecture', 'selftext': 'Hello,   \\nI have worked for a few years as an analyst (self taught) and now I am trying to get into data engineering. I am trying to simply understand how to structure a DWH using\\xa0**medallion architecture**\\xa0(Bronze → Silver → Gold) across\\xa0**multiple environments**\\xa0(Dev / Test / Prod).\\n\\nNow, with the last company I worked with, they simply had two databases, staging, and production. Staging is basically the data lake and they transformed all the data to production. I understand this is not best practice. \\n\\nI thought if I wanted to have a proper structure in my DWH, I was thinking of this:\\n\\nDWH |\\n\\n\\\\-> StagingDB -> BronzeSchema, SilverSchema, GoldSchema\\n\\n\\\\-> TestDB -> BronzeSchema, SilverSchema, GoldSchema\\n\\n\\\\-> ProdDB -> BronzeSchema, SilverSchema, GoldSchema\\n\\nWould you even create a bronze layer on staging and test DBs or not really? I mean it is just the raw data no? ', 'score': 7, 'num_comments': 3, 'author': Redditor(name='Nomad_chh'), 'subreddit_subscribers': 404257, 'created_utc': 1761045229.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1oca4c7/help_a_noob_cicd_pipelines_with_medallion/', 'upvote_ratio': 0.82, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1ocz7qu', 'title': 'What is the best way to orchestrate dbt job in aws', 'selftext': 'I recently joined my company, and they currently run dbt jobs using AWS Step Functions and a Fargate task that executes the project, and so on.\\n\\nHowever, I’m not sure if this is the best approach to orchestrate dbt jobs. Another important point is that the company manages most workflows through events following a DDD (Domain-Driven Design) pattern.\\n\\nRight now, there’s a case where a process depends on two different Step Functions before triggering another process.\\nThe challenge is that these Step Functions run at different times and don’t depend on each other.\\nAdditionally, in the future, there might be other processes that depend on those same Step Functions, but not necessarily on this one\\n\\nIn my opinion, Airflow doesn’t fit well here.\\n\\nWhat do you think would be a better way to manage these processes? Would it make sense to build something more custom for these types of cases', 'score': 4, 'num_comments': 3, 'author': Redditor(name='jonathanrodrigr12'), 'subreddit_subscribers': 404257, 'created_utc': 1761108844.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocz7qu/what_is_the_best_way_to_orchestrate_dbt_job_in_aws/', 'upvote_ratio': 0.71, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1ocalca', 'title': 'Any good PR review tools for data stacks?', 'selftext': 'Has anyone tried using PR review tools like **CodeRabbit** or **Greptile** for data engineering workflows (dbt, Airflow, Snowflake, etc.)?\\n\\nIf anyone can share theier experience on if they can handle things like schema changes, query optimization, or data quality checks well, or if they’re more tuned for general code reviews (which I m mostly expecting).', 'score': 3, 'num_comments': 0, 'author': Redditor(name='Hot_Donkey9172'), 'subreddit_subscribers': 404257, 'created_utc': 1761046763.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocalca/any_good_pr_review_tools_for_data_stacks/', 'upvote_ratio': 0.72, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1od0cqr', 'title': 'Anyone experienced with jOOQ as SQL transpiler?', 'selftext': 'Does anyone have experience with jOOQ (https://github.com/jOOQ/jOOQ) as a transpiler between two different SQL dialects? We are searching for options in Java to run queries from other dialects on Exasol without the users having to rewrite them.', 'score': 2, 'num_comments': 0, 'author': Redditor(name='exagolo'), 'subreddit_subscribers': 404257, 'created_utc': 1761112892.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1od0cqr/anyone_experienced_with_jooq_as_sql_transpiler/', 'upvote_ratio': 0.67, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1ocy398', 'title': 'The Death of Thread Per Core', 'selftext': '', 'score': 2, 'num_comments': 0, 'author': Redditor(name='sionescu'), 'subreddit_subscribers': 404257, 'created_utc': 1761105202.0, 'url': 'https://buttondown.com/jaffray/archive/the-death-of-thread-per-core/', 'upvote_ratio': 0.67, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1ocksmo', 'title': 'Thoughts on Using Synthetic Tabular data for DE projects ?', 'selftext': \"Thoughts on using Synthetic Data for Projects ?\\n\\nI'm currently a DB Specialist with 3 YOE learning Spark, DBT, Python, Airflow and AWS to switch to DE roles.\\n\\nI’d love some feedback on a portfolio project I’m working on. It’s basically a modernized spin on the kind of work I do at my job, a Transaction Data Platform with a multi-step ETL pipeline.\\n\\nQuick overview of setup:\\n\\nDB structure:\\n\\nDimensions = Bank -> Account -> Routing\\n\\nFact = Transactions -> Transaction\\\\_Steps\\n\\nHistory = Hist_Transactions -> Hist_Transaction_Steps (identical to fact tables, just one extra column) \\n\\nI mocked up 3 regions -> 3 banks per region -> 3 accounts per bank -> 702 unique directional routings.\\n\\nA Python script first assigns following parameters to each routing:\\n\\ntype (High Intensity/Frequency/Normal)\\n\\ncountry\\\\_code, region, cross\\\\_border\\n\\nbase\\\\_freq, base\\\\_amount, base\\\\_latency, base\\\\_success\\n\\nvolatility vars (freq/amount/latency/success)\\n\\nThen the synthesizer script uses above paramters to spit out 85k-135k records per day, and 5x times Transaction\\\\_Steps \\n\\nAnomaly engine randomly spikes volatility (50–250x) \\\\~5 times a week for a random routing, the aim is (hopefully) the pipeline will detect the anomalies.\\n\\nPipeline workflow:\\n\\nBatch runs daily (simulating off business hours migration).\\n\\nEvery day data older than 1 month in live table is moved to history tables (partitioned by day and OLTP compressed) \\n\\nThen the partitions older than a month in history tables are exported to Parquet (maybe I'll create a Data lake or something) cold storage and stored. \\n\\nThe current day's transactions are transformed through DBT, to generate 12 marts, helping in anomaly detection and system monitoring \\n\\nA Great Expectation + Python layer takes care of data quality and Anomaly detection\\n\\nFinally for visualization and ease of discussion I'm generating a streamlit dashboard from above 12 marts.\\n\\nMain concerns/questions:\\n\\n1. Since this is just inspired by my current work (I didn’t use real table names/logic, just the concept), should I be worried about IP/overlap ?\\n2. I’ve done a barebones version of this in shell+SQL, so I personally know business and technical requirements and possible issues in this project, it feels really straightforward.  Do you think this is a solid enough project to showcase for DE roles at product-based-companies / fintechs (0–3 YOE range)?\\n3. Thoughts on using synthetic data? I’ve tried to make it noisy and realistic, but since I’ll always have control, I feel like I'm missing something critical that only shows up in real-world messy data?\\n\\nWould love any outside perspective\\n\\nThis would ideally be the portfolio project, and there's one more planned using spark where I'm just cleaning and merging Spotify datasets from different types (CSV, json, sqlite, parquet etc) from Kaggle, it's just a practice project to showcase spark understanding. \\n\\n**TLDR:**  \\nBuilt a synthetic transaction pipeline (750k+ txns, 3.75M steps, anomaly injection, DBT marts, cold storage). Looking for feedback on:\\n\\n* IP concerns (inspired by work but no copied code/keywords)\\n* Whether it’s a strong enough DE project for Product Based Companies and Fintech. \\n* Pros/cons of using synthetic vs real-world messy data\", 'score': 2, 'num_comments': 3, 'author': Redditor(name='Markymark285'), 'subreddit_subscribers': 404257, 'created_utc': 1761071216.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocksmo/thoughts_on_using_synthetic_tabular_data_for_de/', 'upvote_ratio': 0.67, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1ocery5', 'title': 'Anyone else use AWS Redshift Zero-ETL in US-EAST-1?', 'selftext': \"This is a service that basically puts a read replica of an RDS streaming into your Redshift data warehouse. \\n\\nWe have this set up in our environment and it runs many critical systems. After the nightmares of yesterday I checked this morning after getting some complaints from unhappy users about stale data and our ZETL integrations appear to have disappeared entirely. I can see the data and it appears to have stopped updating coincident with yesterday's outage. Looks like I'll have to completely remake these. This is pretty irritating because I can't find any information anywhere from AWS about the outage having *deleted* this infrastructure.\", 'score': 4, 'num_comments': 1, 'author': Redditor(name='bingbongbangchang'), 'subreddit_subscribers': 404257, 'created_utc': 1761057629.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocery5/anyone_else_use_aws_redshift_zeroetl_in_useast1/', 'upvote_ratio': 0.75, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1occmld', 'title': 'DBT unit tests on Redshift', 'selftext': \"Hello!\\n\\nI'm trying to implement unit tests for the DBT models used by my team, so we have more trust on those models and their logic. However I'm getting stuck when the model contains a SUPER-typed column for JSON data.\\n\\nIf I write the JSON object inside a string in the YAML file of the test, then DBT expects unquoted JSON. If I remove the quotes around the JSOn object, then I get a syntax error on the YAML file. I also tried writing the JSON object as a YAML object with indent but it fails too. \\n\\nWhat should I do ?\", 'score': 2, 'num_comments': 5, 'author': Redditor(name='Adrien0623'), 'subreddit_subscribers': 404257, 'created_utc': 1761052451.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1occmld/dbt_unit_tests_on_redshift/', 'upvote_ratio': 0.63, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1od2c18', 'title': 'Data Modeling for the Agentic Era: Semantics, Speed, and Stewardship', 'selftext': '', 'score': 1, 'num_comments': 0, 'author': Redditor(name='sspaeti'), 'subreddit_subscribers': 404257, 'created_utc': 1761120465.0, 'url': 'https://www.rilldata.com/blog/data-modeling-for-the-agentic-era-semantics-speed-and-stewardship', 'upvote_ratio': 0.67, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1ocvodq', 'title': 'Data Streaming Delivery Semantics', 'selftext': 'https://preview.redd.it/h4k1298ikkwf1.png?width=1744&format=png&auto=webp&s=ebf666f0ad1f466edc7e8802e4087f65c35dfd4c\\n\\n[https://codepen.io/gangtao/full/raxdOOK](https://codepen.io/gangtao/full/raxdOOK) ', 'score': 1, 'num_comments': 0, 'author': Redditor(name='gangtao'), 'subreddit_subscribers': 404257, 'created_utc': 1761098042.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocvodq/data_streaming_delivery_semantics/', 'upvote_ratio': 0.67, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1oce5db', 'title': 'Schema Evolution in GCP', 'selftext': 'Hi, i have experience with ELT pipelines on GCP. One that loads data from Postgres tables data into BigQuery and another one that loads CSV files data from SFTP server to BigQuery. \\n\\n* Postgres to BQ:\\n\\nSource -> BQ Bronze layer \\nDataflow (custom template for multi table ingestion in single job)\\n\\nBQ Bronze-> BQ Silver Layer -> BQ Gold \\n\\n————————————————————\\n\\n* SFTP to BQ:\\n\\nSFTP -> GCS\\nAirflow (Composer) SFTP to GCS operator\\n\\nGCS -> BQ Bronze\\nGCS to BQ airflow operator\\n\\nBQ Bronze layer -> BQ Silver Layer -> BQ Gold \\n\\n—————————————————————\\n\\nBQ bronze to silver in both cases is handled using a stored procedure that handles upsert of Bronze data (cleaned data) to Silver. Dataform for Silver to Gold layer.\\n\\nNow, could you please explain how below could be handled in both cases?:\\n\\n1. Schema Evolution \\n2. Handling both incremental or Full load as per requirement using same pipeline (if possible)\\n3. Handling Large Volumes of data (TB) \\n4. Frameworks/tools/methods for Data Quality checks and data validations\\n\\n\\nThank you in advance! \\n\\n\\n', 'score': 1, 'num_comments': 0, 'author': Redditor(name='FeeOk6875'), 'subreddit_subscribers': 404257, 'created_utc': 1761056138.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1oce5db/schema_evolution_in_gcp/', 'upvote_ratio': 0.57, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1ocaik3', 'title': 'Integrating sqlmesh models with Dagster', 'selftext': 'Hey folks,\\n\\nI’m currently using Dagster as the orchestrator in my team’s data stack and I’m considering incorporating sqlmesh as our transformation library. But, I can’t really figure out a way to integrate my sqlmesh models with Dagster so that they show up as individual assets. Has anyone had any luck in achieving this ? How did you go about doing it ?', 'score': 1, 'num_comments': 1, 'author': Redditor(name='YameteGPT'), 'subreddit_subscribers': 404257, 'created_utc': 1761046512.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocaik3/integrating_sqlmesh_models_with_dagster/', 'upvote_ratio': 0.57, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1od0cdl', 'title': 'What do you think it would happen to dbt certificates since it was merged with Fivetran?', 'selftext': 'A bit of context, I am coming from non tech education so I am always looking for ways to learn more about data engineering. \\n\\nI was thinking to prepare for the dbt certificates but since the news came out. I am observing. \\n\\nWhat do you think?', 'score': 0, 'num_comments': 2, 'author': Redditor(name='Ragnareuk'), 'subreddit_subscribers': 404257, 'created_utc': 1761112857.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1od0cdl/what_do_you_think_it_would_happen_to_dbt/', 'upvote_ratio': 0.5, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}, {'id': '1oco9fs', 'title': 'Date time granularity', 'selftext': 'Hi all,\\n\\nI have some room booking data I need to do some time-related calculations with using Power Bi.\\n\\n1st table has room bookings data with room name, meeting start date time, meeting end date time, snapshot\\\\_date, etc.\\n\\nAs part of my ETL I am already building the snapshot\\\\_date rows based on the meeting start date time and meeting end date time.\\n\\n2nd table has room occupancy data which has room name, start date time, stop date time and usage which are in hour buckets.\\n\\nI have a dim date table connected to snapshot\\\\_date in the room bookings table and start date time in the room occupancy table.\\n\\nQuestion is do I need to have my room bookings data at the same time granularity (hourly) as the room occupancy data to make life easier with time calculations moving forward.\\n\\nCheers', 'score': 0, 'num_comments': 2, 'author': Redditor(name='ImFizzyGoodNice'), 'subreddit_subscribers': 404257, 'created_utc': 1761078928.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1oco9fs/date_time_granularity/', 'upvote_ratio': 0.4, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}]","logger":"task.stdout"}
{"timestamp":"2025-10-22T10:23:03.320469Z","level":"info","event":"head         id  ... stickied","logger":"task.stdout"}
{"timestamp":"2025-10-22T10:23:03.320876Z","level":"info","event":"0  1occxjl  ...    False","logger":"task.stdout"}
{"timestamp":"2025-10-22T10:23:03.321066Z","level":"info","event":"1  1ocst3a  ...    False","logger":"task.stdout"}
{"timestamp":"2025-10-22T10:23:03.321179Z","level":"info","event":"2  1ocie74  ...    False","logger":"task.stdout"}
{"timestamp":"2025-10-22T10:23:03.321356Z","level":"info","event":"3  1od2x2k  ...    False","logger":"task.stdout"}
{"timestamp":"2025-10-22T10:23:03.321469Z","level":"info","event":"4  1ocqznw  ...    False","logger":"task.stdout"}
{"timestamp":"2025-10-22T10:23:03.321602Z","level":"info","event":"","logger":"task.stdout"}
{"timestamp":"2025-10-22T10:23:03.321697Z","level":"info","event":"[5 rows x 14 columns]","logger":"task.stdout"}
{"timestamp":"2025-10-22T10:23:03.321779Z","level":"info","event":"Index(['id', 'title', 'selftext', 'score', 'num_comments', 'author',","logger":"task.stdout"}
{"timestamp":"2025-10-22T10:23:03.321850Z","level":"info","event":"       'subreddit_subscribers', 'created_utc', 'url', 'upvote_ratio',","logger":"task.stdout"}
{"timestamp":"2025-10-22T10:23:03.321936Z","level":"info","event":"       'over_18', 'edited', 'spoiler', 'stickied'],","logger":"task.stdout"}
{"timestamp":"2025-10-22T10:23:03.322043Z","level":"info","event":"      dtype='object')","logger":"task.stdout"}
{"timestamp":"2025-10-22T10:23:03.331854Z","level":"info","event":"Done. Returned value was: None","logger":"airflow.task.operators.airflow.providers.standard.operators.python.PythonOperator","filename":"python.py","lineno":218}
