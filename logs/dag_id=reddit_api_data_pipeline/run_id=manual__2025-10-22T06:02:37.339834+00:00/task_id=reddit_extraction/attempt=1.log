{"timestamp":"2025-10-22T06:02:42.319110Z","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager","filename":"manager.py","lineno":179}
{"timestamp":"2025-10-22T06:02:42.320870Z","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/main_dag.py","logger":"airflow.models.dagbag.DagBag","filename":"dagbag.py","lineno":593}
{"timestamp":"2025-10-22T06:02:42.879469Z","level":"info","event":"connected to reddit!","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:42.879971Z","level":"info","event":"extract popst 1","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:42.880683Z","level":"info","event":"extract popst 2","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:42.881402Z","level":"info","event":"<praw.models.listing.generator.ListingGenerator object at 0x7f42ec78a210>","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.205658Z","level":"info","event":"error?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.206418Z","level":"info","event":"error 2?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.207097Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f42ec9f1c70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Classic story: you should not work directly in prod, but apply the best devops practices, develop data pipelines in a development environment, debug, deploy in pre-prod, test, then deploy in production.\\n\\nWhat about data analysts? data scientists? statisticians? ad-hoc reports?\\n\\nMost data books focus on the data engineering lifecycle, sometimes they talk about the \"Analytics sandbox\", but they rarely address heads-on the personas doing analytics work in production. Modern data platform allow the decoupling of compute and data, enabling workload isolation to allow users read-only access to production data without affecting production workloads. Other teams perform data replication from production to lower environments. There\\'s also the \"blue-green development architecture\", with two systems with production data.\\n\\nHow are you dealing with users requesting production data?', 'author_fullname': 't2_kj30g', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Developing with production data: who and how?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1occxjl', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.86, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 18, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 18, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1761053214.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Classic story: you should not work directly in prod, but apply the best devops practices, develop data pipelines in a development environment, debug, deploy in pre-prod, test, then deploy in production.</p>\\n\\n<p>What about data analysts? data scientists? statisticians? ad-hoc reports?</p>\\n\\n<p>Most data books focus on the data engineering lifecycle, sometimes they talk about the &quot;Analytics sandbox&quot;, but they rarely address heads-on the personas doing analytics work in production. Modern data platform allow the decoupling of compute and data, enabling workload isolation to allow users read-only access to production data without affecting production workloads. Other teams perform data replication from production to lower environments. There&#39;s also the &quot;blue-green development architecture&quot;, with two systems with production data.</p>\\n\\n<p>How are you dealing with users requesting production data?</p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1occxjl', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='aburkh'), 'discussion_type': None, 'num_comments': 24, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1occxjl/developing_with_production_data_who_and_how/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1occxjl/developing_with_production_data_who_and_how/', 'subreddit_subscribers': 404232, 'created_utc': 1761053214.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.208337Z","level":"info","event":"error?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.208905Z","level":"info","event":"error 2?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.210059Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f42ec9f1c70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey all I could really use some career advice from this community.\\n\\nI was fortunate to land 2 offers in this market, but now I’m struggling to make the right long term decision.\\n\\nI’m finishing my Master’s in Data Science next semester. I interned last summer at a big company and then started working in my first FT data role as a data analyst at a small company (I’m about 6 months in). My goal is to eventually move into Data Science/ML maybe ML engineer and end up in big tech.\\n\\nOption A: Data Engineer I\\n* Industry: Finance. This one pays $15k more. I’ll be working with a smaller team and I’d be the main technical person on the team. So no strong mentorship and I’ll have the pressure to “figure it out” on my own. \\n\\nOption B: Senior Data Analyst\\n* Industry: retail at a large org.\\n\\nI’m nervous about being the only engineer on a team this early in my career…But I’m also worried about not being technical enough as a data analyst and not being technical. \\n\\nWhat would you do in my shoes?\\nGo hard into engineering now and level up fast even if it’s stressful without much support? Or take the analyst role at a big company, build brand and transition later?\\n\\nWould appreciate any advice from people who’ve been on either path.\\n', 'author_fullname': 't2_roc5as6y8', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Need advice choosing between Data engineer vs Sr Data analyst', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ocst3a', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.85, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 9, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 9, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1761090065.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Hey all I could really use some career advice from this community.</p>\\n\\n<p>I was fortunate to land 2 offers in this market, but now I’m struggling to make the right long term decision.</p>\\n\\n<p>I’m finishing my Master’s in Data Science next semester. I interned last summer at a big company and then started working in my first FT data role as a data analyst at a small company (I’m about 6 months in). My goal is to eventually move into Data Science/ML maybe ML engineer and end up in big tech.</p>\\n\\n<p>Option A: Data Engineer I\\n* Industry: Finance. This one pays $15k more. I’ll be working with a smaller team and I’d be the main technical person on the team. So no strong mentorship and I’ll have the pressure to “figure it out” on my own. </p>\\n\\n<p>Option B: Senior Data Analyst\\n* Industry: retail at a large org.</p>\\n\\n<p>I’m nervous about being the only engineer on a team this early in my career…But I’m also worried about not being technical enough as a data analyst and not being technical. </p>\\n\\n<p>What would you do in my shoes?\\nGo hard into engineering now and level up fast even if it’s stressful without much support? Or take the analyst role at a big company, build brand and transition later?</p>\\n\\n<p>Would appreciate any advice from people who’ve been on either path.</p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1ocst3a', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Agile_Yak3819'), 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ocst3a/need_advice_choosing_between_data_engineer_vs_sr/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocst3a/need_advice_choosing_between_data_engineer_vs_sr/', 'subreddit_subscribers': 404232, 'created_utc': 1761090065.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.210970Z","level":"info","event":"error?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.211322Z","level":"info","event":"error 2?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.211935Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f42ec9f1c70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': \"Hope y'all find it useful!\", 'author_fullname': 't2_11in8zc3ls', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Our 7 Snowflake query optimization tips and why they work', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 105, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ocie74', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.85, 'author_flair_background_color': None, 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://external-preview.redd.it/fH0SwQb9KzwejcwGUKY6uYFMij2Eczgx-xl0LhKHzIs.png?width=140&height=105&auto=webp&s=a61dab94c3bcc4af1871622f9f01fae358244635', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1761065889.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'blog.greybeam.ai', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Hope y&#39;all find it useful!</p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://blog.greybeam.ai/snowflake-query-optimization/', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/fH0SwQb9KzwejcwGUKY6uYFMij2Eczgx-xl0LhKHzIs.png?auto=webp&s=f6a0f223cd323de72392223aa24422c74d7d6684', 'width': 1200, 'height': 900}, 'resolutions': [{'url': 'https://external-preview.redd.it/fH0SwQb9KzwejcwGUKY6uYFMij2Eczgx-xl0LhKHzIs.png?width=108&crop=smart&auto=webp&s=426c74c16a1f1c543053b8527607756350294ce4', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/fH0SwQb9KzwejcwGUKY6uYFMij2Eczgx-xl0LhKHzIs.png?width=216&crop=smart&auto=webp&s=560d563496913d5e5571dfc0be2a6649dab296b8', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/fH0SwQb9KzwejcwGUKY6uYFMij2Eczgx-xl0LhKHzIs.png?width=320&crop=smart&auto=webp&s=c07561da42849f080393e0bec4c947fa68fe78af', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/fH0SwQb9KzwejcwGUKY6uYFMij2Eczgx-xl0LhKHzIs.png?width=640&crop=smart&auto=webp&s=b1e1d509ec49771137fdd77f3ed51b4aa85df189', 'width': 640, 'height': 480}, {'url': 'https://external-preview.redd.it/fH0SwQb9KzwejcwGUKY6uYFMij2Eczgx-xl0LhKHzIs.png?width=960&crop=smart&auto=webp&s=b0357529b9ce244bd0ef7eee1f697f3512950a65', 'width': 960, 'height': 720}, {'url': 'https://external-preview.redd.it/fH0SwQb9KzwejcwGUKY6uYFMij2Eczgx-xl0LhKHzIs.png?width=1080&crop=smart&auto=webp&s=d13eca663de412329cdba0a08d6e372ff731b65d', 'width': 1080, 'height': 810}], 'variants': {}, 'id': 'fH0SwQb9KzwejcwGUKY6uYFMij2Eczgx-xl0LhKHzIs'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1ocie74', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='hornyforsavings'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ocie74/our_7_snowflake_query_optimization_tips_and_why/', 'stickied': False, 'url': 'https://blog.greybeam.ai/snowflake-query-optimization/', 'subreddit_subscribers': 404232, 'created_utc': 1761065889.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.212693Z","level":"info","event":"error?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.213102Z","level":"info","event":"error 2?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.210597Z","level":"info","event":"Done. Returned value was: None","logger":"airflow.task.operators.airflow.providers.standard.operators.python.PythonOperator","filename":"python.py","lineno":218}
{"timestamp":"2025-10-22T06:02:44.236909Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f42ec9f1c70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Has anyone tried using PR review tools like **CodeRabbit** or **Greptile** for data engineering workflows (dbt, Airflow, Snowflake, etc.)?\\n\\nIf anyone can share theier experience on if they can handle things like schema changes, query optimization, or data quality checks well, or if they’re more tuned for general code reviews (which I m mostly expecting).', 'author_fullname': 't2_1909u2flfw', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Any good PR review tools for data stacks?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ocalca', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.72, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1761046763.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Has anyone tried using PR review tools like <strong>CodeRabbit</strong> or <strong>Greptile</strong> for data engineering workflows (dbt, Airflow, Snowflake, etc.)?</p>\\n\\n<p>If anyone can share theier experience on if they can handle things like schema changes, query optimization, or data quality checks well, or if they’re more tuned for general code reviews (which I m mostly expecting).</p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ocalca', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Hot_Donkey9172'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ocalca/any_good_pr_review_tools_for_data_stacks/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocalca/any_good_pr_review_tools_for_data_stacks/', 'subreddit_subscribers': 404232, 'created_utc': 1761046763.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.237291Z","level":"info","event":"error?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.237493Z","level":"info","event":"error 2?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.237685Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f42ec9f1c70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Has anyone ever had any success using tools like DataBricks Lakebridge or Snowflake\\'s SnowConvert to migrate Informatica powercenter ETL pipelines to another platform? I assume at best they \"kind of work sometimes for some things\" but am curious to hear anyone\\'s actual experience with them in the wild.', 'author_fullname': 't2_3q4crsje', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Tools for automated migration away from Informatica', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ocqznw', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1761085359.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Has anyone ever had any success using tools like DataBricks Lakebridge or Snowflake&#39;s SnowConvert to migrate Informatica powercenter ETL pipelines to another platform? I assume at best they &quot;kind of work sometimes for some things&quot; but am curious to hear anyone&#39;s actual experience with them in the wild.</p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ocqznw', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='BadKafkaPartitioning'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ocqznw/tools_for_automated_migration_away_from/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocqznw/tools_for_automated_migration_away_from/', 'subreddit_subscribers': 404232, 'created_utc': 1761085359.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.237925Z","level":"info","event":"error?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.238745Z","level":"info","event":"error 2?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.240881Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f42ec9f1c70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hello,   \\nI have worked for a few years as an analyst (self taught) and now I am trying to get into data engineering. I am trying to simply understand how to structure a DWH using\\xa0**medallion architecture**\\xa0(Bronze → Silver → Gold) across\\xa0**multiple environments**\\xa0(Dev / Test / Prod).\\n\\nNow, with the last company I worked with, they simply had two databases, staging, and production. Staging is basically the data lake and they transformed all the data to production. I understand this is not best practice. \\n\\nI thought if I wanted to have a proper structure in my DWH, I was thinking of this:\\n\\nDWH |\\n\\n\\\\-> StagingDB -> BronzeSchema, SilverSchema, GoldSchema\\n\\n\\\\-> TestDB -> BronzeSchema, SilverSchema, GoldSchema\\n\\n\\\\-> ProdDB -> BronzeSchema, SilverSchema, GoldSchema\\n\\nWould you even create a bronze layer on staging and test DBs or not really? I mean it is just the raw data no? ', 'author_fullname': 't2_t4ios3gq', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Help a noob: CI/CD pipelines with medallion architecture', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1oca4c7', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.64, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1761045229.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Hello,<br/>\\nI have worked for a few years as an analyst (self taught) and now I am trying to get into data engineering. I am trying to simply understand how to structure a DWH using\\xa0<strong>medallion architecture</strong>\\xa0(Bronze → Silver → Gold) across\\xa0<strong>multiple environments</strong>\\xa0(Dev / Test / Prod).</p>\\n\\n<p>Now, with the last company I worked with, they simply had two databases, staging, and production. Staging is basically the data lake and they transformed all the data to production. I understand this is not best practice. </p>\\n\\n<p>I thought if I wanted to have a proper structure in my DWH, I was thinking of this:</p>\\n\\n<p>DWH |</p>\\n\\n<p>-&gt; StagingDB -&gt; BronzeSchema, SilverSchema, GoldSchema</p>\\n\\n<p>-&gt; TestDB -&gt; BronzeSchema, SilverSchema, GoldSchema</p>\\n\\n<p>-&gt; ProdDB -&gt; BronzeSchema, SilverSchema, GoldSchema</p>\\n\\n<p>Would you even create a bronze layer on staging and test DBs or not really? I mean it is just the raw data no? </p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1oca4c7', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Nomad_chh'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1oca4c7/help_a_noob_cicd_pipelines_with_medallion/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1oca4c7/help_a_noob_cicd_pipelines_with_medallion/', 'subreddit_subscribers': 404232, 'created_utc': 1761045229.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.241220Z","level":"info","event":"error?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.241375Z","level":"info","event":"error 2?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.241576Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f42ec9f1c70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': \"HI All, \\n\\nI worked as a Data Analyst (Mid Level) and had to take a career break for 4 years (Just as COVID took over). From the past year, I have been taking DE course on LinkedIn/ Udemy and looking to restart my career. Though I've applied for number of jobs  since April this year, I did not get any calls , not even a preliminary screening call. I understand the economy is not doing so good currently, but I'm desperate for a job. Any suggestions on how to land a DE job? Thanks \", 'author_fullname': 't2_1lvh6y4kg6', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Looking for advice', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ocw51f', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.8, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1761099380.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>HI All, </p>\\n\\n<p>I worked as a Data Analyst (Mid Level) and had to take a career break for 4 years (Just as COVID took over). From the past year, I have been taking DE course on LinkedIn/ Udemy and looking to restart my career. Though I&#39;ve applied for number of jobs  since April this year, I did not get any calls , not even a preliminary screening call. I understand the economy is not doing so good currently, but I&#39;m desperate for a job. Any suggestions on how to land a DE job? Thanks </p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1ocw51f', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='EngineeringFar2638'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ocw51f/looking_for_advice/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocw51f/looking_for_advice/', 'subreddit_subscribers': 404232, 'created_utc': 1761099380.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.241791Z","level":"info","event":"error?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.241934Z","level":"info","event":"error 2?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.242097Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f42ec9f1c70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': \"Thoughts on using Synthetic Data for Projects ?\\n\\nI'm currently a DB Specialist with 3 YOE learning Spark, DBT, Python, Airflow and AWS to switch to DE roles.\\n\\nI’d love some feedback on a portfolio project I’m working on. It’s basically a modernized spin on the kind of work I do at my job, a Transaction Data Platform with a multi-step ETL pipeline.\\n\\nQuick overview of setup:\\n\\nDB structure:\\n\\nDimensions = Bank -> Account -> Routing\\n\\nFact = Transactions -> Transaction\\\\_Steps\\n\\nHistory = Hist_Transactions -> Hist_Transaction_Steps (identical to fact tables, just one extra column) \\n\\nI mocked up 3 regions -> 3 banks per region -> 3 accounts per bank -> 702 unique directional routings.\\n\\nA Python script first assigns following parameters to each routing:\\n\\ntype (High Intensity/Frequency/Normal)\\n\\ncountry\\\\_code, region, cross\\\\_border\\n\\nbase\\\\_freq, base\\\\_amount, base\\\\_latency, base\\\\_success\\n\\nvolatility vars (freq/amount/latency/success)\\n\\nThen the synthesizer script uses above paramters to spit out 85k-135k records per day, and 5x times Transaction\\\\_Steps \\n\\nAnomaly engine randomly spikes volatility (50–250x) \\\\~5 times a week for a random routing, the aim is (hopefully) the pipeline will detect the anomalies.\\n\\nPipeline workflow:\\n\\nBatch runs daily (simulating off business hours migration).\\n\\nEvery day data older than 1 month in live table is moved to history tables (partitioned by day and OLTP compressed) \\n\\nThen the partitions older than a month in history tables are exported to Parquet (maybe I'll create a Data lake or something) cold storage and stored. \\n\\nThe current day's transactions are transformed through DBT, to generate 12 marts, helping in anomaly detection and system monitoring \\n\\nA Great Expectation + Python layer takes care of data quality and Anomaly detection\\n\\nFinally for visualization and ease of discussion I'm generating a streamlit dashboard from above 12 marts.\\n\\nMain concerns/questions:\\n\\n1. Since this is just inspired by my current work (I didn’t use real table names/logic, just the concept), should I be worried about IP/overlap ?\\n2. I’ve done a barebones version of this in shell+SQL, so I personally know business and technical requirements and possible issues in this project, it feels really straightforward.  Do you think this is a solid enough project to showcase for DE roles at product-based-companies / fintechs (0–3 YOE range)?\\n3. Thoughts on using synthetic data? I’ve tried to make it noisy and realistic, but since I’ll always have control, I feel like I'm missing something critical that only shows up in real-world messy data?\\n\\nWould love any outside perspective\\n\\nThis would ideally be the portfolio project, and there's one more planned using spark where I'm just cleaning and merging Spotify datasets from different types (CSV, json, sqlite, parquet etc) from Kaggle, it's just a practice project to showcase spark understanding. \\n\\n**TLDR:**  \\nBuilt a synthetic transaction pipeline (750k+ txns, 3.75M steps, anomaly injection, DBT marts, cold storage). Looking for feedback on:\\n\\n* IP concerns (inspired by work but no copied code/keywords)\\n* Whether it’s a strong enough DE project for Product Based Companies and Fintech. \\n* Pros/cons of using synthetic vs real-world messy data\", 'author_fullname': 't2_px9t1wx0t', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Thoughts on Using Synthetic Tabular data for DE projects ?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ocksmo', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1761071216.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Thoughts on using Synthetic Data for Projects ?</p>\\n\\n<p>I&#39;m currently a DB Specialist with 3 YOE learning Spark, DBT, Python, Airflow and AWS to switch to DE roles.</p>\\n\\n<p>I’d love some feedback on a portfolio project I’m working on. It’s basically a modernized spin on the kind of work I do at my job, a Transaction Data Platform with a multi-step ETL pipeline.</p>\\n\\n<p>Quick overview of setup:</p>\\n\\n<p>DB structure:</p>\\n\\n<p>Dimensions = Bank -&gt; Account -&gt; Routing</p>\\n\\n<p>Fact = Transactions -&gt; Transaction_Steps</p>\\n\\n<p>History = Hist_Transactions -&gt; Hist_Transaction_Steps (identical to fact tables, just one extra column) </p>\\n\\n<p>I mocked up 3 regions -&gt; 3 banks per region -&gt; 3 accounts per bank -&gt; 702 unique directional routings.</p>\\n\\n<p>A Python script first assigns following parameters to each routing:</p>\\n\\n<p>type (High Intensity/Frequency/Normal)</p>\\n\\n<p>country_code, region, cross_border</p>\\n\\n<p>base_freq, base_amount, base_latency, base_success</p>\\n\\n<p>volatility vars (freq/amount/latency/success)</p>\\n\\n<p>Then the synthesizer script uses above paramters to spit out 85k-135k records per day, and 5x times Transaction_Steps </p>\\n\\n<p>Anomaly engine randomly spikes volatility (50–250x) ~5 times a week for a random routing, the aim is (hopefully) the pipeline will detect the anomalies.</p>\\n\\n<p>Pipeline workflow:</p>\\n\\n<p>Batch runs daily (simulating off business hours migration).</p>\\n\\n<p>Every day data older than 1 month in live table is moved to history tables (partitioned by day and OLTP compressed) </p>\\n\\n<p>Then the partitions older than a month in history tables are exported to Parquet (maybe I&#39;ll create a Data lake or something) cold storage and stored. </p>\\n\\n<p>The current day&#39;s transactions are transformed through DBT, to generate 12 marts, helping in anomaly detection and system monitoring </p>\\n\\n<p>A Great Expectation + Python layer takes care of data quality and Anomaly detection</p>\\n\\n<p>Finally for visualization and ease of discussion I&#39;m generating a streamlit dashboard from above 12 marts.</p>\\n\\n<p>Main concerns/questions:</p>\\n\\n<ol>\\n<li>Since this is just inspired by my current work (I didn’t use real table names/logic, just the concept), should I be worried about IP/overlap ?</li>\\n<li>I’ve done a barebones version of this in shell+SQL, so I personally know business and technical requirements and possible issues in this project, it feels really straightforward.  Do you think this is a solid enough project to showcase for DE roles at product-based-companies / fintechs (0–3 YOE range)?</li>\\n<li>Thoughts on using synthetic data? I’ve tried to make it noisy and realistic, but since I’ll always have control, I feel like I&#39;m missing something critical that only shows up in real-world messy data?</li>\\n</ol>\\n\\n<p>Would love any outside perspective</p>\\n\\n<p>This would ideally be the portfolio project, and there&#39;s one more planned using spark where I&#39;m just cleaning and merging Spotify datasets from different types (CSV, json, sqlite, parquet etc) from Kaggle, it&#39;s just a practice project to showcase spark understanding. </p>\\n\\n<p><strong>TLDR:</strong><br/>\\nBuilt a synthetic transaction pipeline (750k+ txns, 3.75M steps, anomaly injection, DBT marts, cold storage). Looking for feedback on:</p>\\n\\n<ul>\\n<li>IP concerns (inspired by work but no copied code/keywords)</li>\\n<li>Whether it’s a strong enough DE project for Product Based Companies and Fintech. </li>\\n<li>Pros/cons of using synthetic vs real-world messy data</li>\\n</ul>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ocksmo', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Markymark285'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ocksmo/thoughts_on_using_synthetic_tabular_data_for_de/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocksmo/thoughts_on_using_synthetic_tabular_data_for_de/', 'subreddit_subscribers': 404232, 'created_utc': 1761071216.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.242336Z","level":"info","event":"error?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.245971Z","level":"info","event":"error 2?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.246637Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f42ec9f1c70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': \"This is a service that basically puts a read replica of an RDS streaming into your Redshift data warehouse. \\n\\nWe have this set up in our environment and it runs many critical systems. After the nightmares of yesterday I checked this morning after getting some complaints from unhappy users about stale data and our ZETL integrations appear to have disappeared entirely. I can see the data and it appears to have stopped updating coincident with yesterday's outage. Looks like I'll have to completely remake these. This is pretty irritating because I can't find any information anywhere from AWS about the outage having *deleted* this infrastructure.\", 'author_fullname': 't2_1kqjly49dr', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Anyone else use AWS Redshift Zero-ETL in US-EAST-1?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ocery5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1761057629.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>This is a service that basically puts a read replica of an RDS streaming into your Redshift data warehouse. </p>\\n\\n<p>We have this set up in our environment and it runs many critical systems. After the nightmares of yesterday I checked this morning after getting some complaints from unhappy users about stale data and our ZETL integrations appear to have disappeared entirely. I can see the data and it appears to have stopped updating coincident with yesterday&#39;s outage. Looks like I&#39;ll have to completely remake these. This is pretty irritating because I can&#39;t find any information anywhere from AWS about the outage having <em>deleted</em> this infrastructure.</p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': True, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ocery5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='bingbongbangchang'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ocery5/anyone_else_use_aws_redshift_zeroetl_in_useast1/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocery5/anyone_else_use_aws_redshift_zeroetl_in_useast1/', 'subreddit_subscribers': 404232, 'created_utc': 1761057629.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.246927Z","level":"info","event":"error?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.248773Z","level":"info","event":"error 2?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.249634Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f42ec9f1c70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': \"Hello!\\n\\nI'm trying to implement unit tests for the DBT models used by my team, so we have more trust on those models and their logic. However I'm getting stuck when the model contains a SUPER-typed column for JSON data.\\n\\nIf I write the JSON object inside a string in the YAML file of the test, then DBT expects unquoted JSON. If I remove the quotes around the JSOn object, then I get a syntax error on the YAML file. I also tried writing the JSON object as a YAML object with indent but it fails too. \\n\\nWhat should I do ?\", 'author_fullname': 't2_w0ki78zt', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'DBT unit tests on Redshift', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1occmld', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.63, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1761052451.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Hello!</p>\\n\\n<p>I&#39;m trying to implement unit tests for the DBT models used by my team, so we have more trust on those models and their logic. However I&#39;m getting stuck when the model contains a SUPER-typed column for JSON data.</p>\\n\\n<p>If I write the JSON object inside a string in the YAML file of the test, then DBT expects unquoted JSON. If I remove the quotes around the JSOn object, then I get a syntax error on the YAML file. I also tried writing the JSON object as a YAML object with indent but it fails too. </p>\\n\\n<p>What should I do ?</p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1occmld', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Adrien0623'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1occmld/dbt_unit_tests_on_redshift/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1occmld/dbt_unit_tests_on_redshift/', 'subreddit_subscribers': 404232, 'created_utc': 1761052451.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.251875Z","level":"info","event":"error?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.252231Z","level":"info","event":"error 2?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.252635Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f42ec9f1c70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_7jm9e', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'The Death of Thread Per Core', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 91, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ocy398', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': True, 'thumbnail': 'https://external-preview.redd.it/xViabq6iz6fSNoA0SmrJGnulDzwHlbSwjri6WzEk5dw.png?width=140&height=91&auto=webp&s=bcf7456e787d91fe14f24f4006e7607120c349fc', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1761105202.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'buttondown.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://buttondown.com/jaffray/archive/the-death-of-thread-per-core/', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/xViabq6iz6fSNoA0SmrJGnulDzwHlbSwjri6WzEk5dw.png?auto=webp&s=e1d62e38619d786023558c2a5e7fc1849afa982c', 'width': 500, 'height': 328}, 'resolutions': [{'url': 'https://external-preview.redd.it/xViabq6iz6fSNoA0SmrJGnulDzwHlbSwjri6WzEk5dw.png?width=108&crop=smart&auto=webp&s=c97b08bd7b42e46f27b64ebc2e867a3569182a02', 'width': 108, 'height': 70}, {'url': 'https://external-preview.redd.it/xViabq6iz6fSNoA0SmrJGnulDzwHlbSwjri6WzEk5dw.png?width=216&crop=smart&auto=webp&s=7b66425abb7d8a741a4c254d375a2c52fad87a8c', 'width': 216, 'height': 141}, {'url': 'https://external-preview.redd.it/xViabq6iz6fSNoA0SmrJGnulDzwHlbSwjri6WzEk5dw.png?width=320&crop=smart&auto=webp&s=462df7faa33616ae962a902f12e22608e5ee7a14', 'width': 320, 'height': 209}], 'variants': {}, 'id': 'xViabq6iz6fSNoA0SmrJGnulDzwHlbSwjri6WzEk5dw'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1ocy398', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='sionescu'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ocy398/the_death_of_thread_per_core/', 'stickied': False, 'url': 'https://buttondown.com/jaffray/archive/the-death-of-thread-per-core/', 'subreddit_subscribers': 404232, 'created_utc': 1761105202.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.252905Z","level":"info","event":"error?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.253062Z","level":"info","event":"error 2?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.253804Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f42ec9f1c70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'https://preview.redd.it/h4k1298ikkwf1.png?width=1744&format=png&auto=webp&s=ebf666f0ad1f466edc7e8802e4087f65c35dfd4c\\n\\n[https://codepen.io/gangtao/full/raxdOOK](https://codepen.io/gangtao/full/raxdOOK) ', 'author_fullname': 't2_g96mx9eu', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data Streaming Delivery Semantics', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 92, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'h4k1298ikkwf1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 71, 'x': 108, 'u': 'https://preview.redd.it/h4k1298ikkwf1.png?width=108&crop=smart&auto=webp&s=a1d9cf5f6375e36480d8f29fb50d8f7bf3d90333'}, {'y': 143, 'x': 216, 'u': 'https://preview.redd.it/h4k1298ikkwf1.png?width=216&crop=smart&auto=webp&s=bdbcc54601356ed06082a376334a446325607d81'}, {'y': 212, 'x': 320, 'u': 'https://preview.redd.it/h4k1298ikkwf1.png?width=320&crop=smart&auto=webp&s=70906d0c02f76bbc09e338ac9e3652ae839095f5'}, {'y': 424, 'x': 640, 'u': 'https://preview.redd.it/h4k1298ikkwf1.png?width=640&crop=smart&auto=webp&s=06f83831de284a5587823cb524de7d2bb4db8299'}, {'y': 637, 'x': 960, 'u': 'https://preview.redd.it/h4k1298ikkwf1.png?width=960&crop=smart&auto=webp&s=2e8849b9a58d3c7744f2399bd4f18ceeab8308ee'}, {'y': 717, 'x': 1080, 'u': 'https://preview.redd.it/h4k1298ikkwf1.png?width=1080&crop=smart&auto=webp&s=941bf98a9c551b1722b5295f219f4a7cdf707dbb'}], 's': {'y': 1158, 'x': 1744, 'u': 'https://preview.redd.it/h4k1298ikkwf1.png?width=1744&format=png&auto=webp&s=ebf666f0ad1f466edc7e8802e4087f65c35dfd4c'}, 'id': 'h4k1298ikkwf1'}}, 'name': 't3_1ocvodq', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/PkzaVOWug2nNK14JMBPirji7rOT6r6GT-U_ZKineZt8.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1761098042.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://preview.redd.it/h4k1298ikkwf1.png?width=1744&amp;format=png&amp;auto=webp&amp;s=ebf666f0ad1f466edc7e8802e4087f65c35dfd4c\">https://preview.redd.it/h4k1298ikkwf1.png?width=1744&amp;format=png&amp;auto=webp&amp;s=ebf666f0ad1f466edc7e8802e4087f65c35dfd4c</a></p>\\n\\n<p><a href=\"https://codepen.io/gangtao/full/raxdOOK\">https://codepen.io/gangtao/full/raxdOOK</a> </p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1ocvodq', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='gangtao'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ocvodq/data_streaming_delivery_semantics/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocvodq/data_streaming_delivery_semantics/', 'subreddit_subscribers': 404232, 'created_utc': 1761098042.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.254306Z","level":"info","event":"error?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.254538Z","level":"info","event":"error 2?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.254779Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f42ec9f1c70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi, i have experience with ELT pipelines on GCP. One that loads data from Postgres tables data into BigQuery and another one that loads CSV files data from SFTP server to BigQuery. \\n\\n* Postgres to BQ:\\n\\nSource -> BQ Bronze layer \\nDataflow (custom template for multi table ingestion in single job)\\n\\nBQ Bronze-> BQ Silver Layer -> BQ Gold \\n\\n————————————————————\\n\\n* SFTP to BQ:\\n\\nSFTP -> GCS\\nAirflow (Composer) SFTP to GCS operator\\n\\nGCS -> BQ Bronze\\nGCS to BQ airflow operator\\n\\nBQ Bronze layer -> BQ Silver Layer -> BQ Gold \\n\\n—————————————————————\\n\\nBQ bronze to silver in both cases is handled using a stored procedure that handles upsert of Bronze data (cleaned data) to Silver. Dataform for Silver to Gold layer.\\n\\nNow, could you please explain how below could be handled in both cases?:\\n\\n1. Schema Evolution \\n2. Handling both incremental or Full load as per requirement using same pipeline (if possible)\\n3. Handling Large Volumes of data (TB) \\n4. Frameworks/tools/methods for Data Quality checks and data validations\\n\\n\\nThank you in advance! \\n\\n\\n', 'author_fullname': 't2_126kk9oso1', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Schema Evolution in GCP', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1oce5db', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1761056138.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Hi, i have experience with ELT pipelines on GCP. One that loads data from Postgres tables data into BigQuery and another one that loads CSV files data from SFTP server to BigQuery. </p>\\n\\n<ul>\\n<li>Postgres to BQ:</li>\\n</ul>\\n\\n<p>Source -&gt; BQ Bronze layer \\nDataflow (custom template for multi table ingestion in single job)</p>\\n\\n<p>BQ Bronze-&gt; BQ Silver Layer -&gt; BQ Gold </p>\\n\\n<p>————————————————————</p>\\n\\n<ul>\\n<li>SFTP to BQ:</li>\\n</ul>\\n\\n<p>SFTP -&gt; GCS\\nAirflow (Composer) SFTP to GCS operator</p>\\n\\n<p>GCS -&gt; BQ Bronze\\nGCS to BQ airflow operator</p>\\n\\n<p>BQ Bronze layer -&gt; BQ Silver Layer -&gt; BQ Gold </p>\\n\\n<p>—————————————————————</p>\\n\\n<p>BQ bronze to silver in both cases is handled using a stored procedure that handles upsert of Bronze data (cleaned data) to Silver. Dataform for Silver to Gold layer.</p>\\n\\n<p>Now, could you please explain how below could be handled in both cases?:</p>\\n\\n<ol>\\n<li>Schema Evolution </li>\\n<li>Handling both incremental or Full load as per requirement using same pipeline (if possible)</li>\\n<li>Handling Large Volumes of data (TB) </li>\\n<li>Frameworks/tools/methods for Data Quality checks and data validations</li>\\n</ol>\\n\\n<p>Thank you in advance! </p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1oce5db', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='FeeOk6875'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1oce5db/schema_evolution_in_gcp/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1oce5db/schema_evolution_in_gcp/', 'subreddit_subscribers': 404232, 'created_utc': 1761056138.0, 'num_crossposts': 1, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.255167Z","level":"info","event":"error?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.255745Z","level":"info","event":"error 2?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.256085Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f42ec9f1c70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey folks,\\n\\nI’m currently using Dagster as the orchestrator in my team’s data stack and I’m considering incorporating sqlmesh as our transformation library. But, I can’t really figure out a way to integrate my sqlmesh models with Dagster so that they show up as individual assets. Has anyone had any luck in achieving this ? How did you go about doing it ?', 'author_fullname': 't2_5nb5elgdm', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Integrating sqlmesh models with Dagster', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ocaik3', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.57, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1761046512.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Hey folks,</p>\\n\\n<p>I’m currently using Dagster as the orchestrator in my team’s data stack and I’m considering incorporating sqlmesh as our transformation library. But, I can’t really figure out a way to integrate my sqlmesh models with Dagster so that they show up as individual assets. Has anyone had any luck in achieving this ? How did you go about doing it ?</p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ocaik3', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='YameteGPT'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ocaik3/integrating_sqlmesh_models_with_dagster/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocaik3/integrating_sqlmesh_models_with_dagster/', 'subreddit_subscribers': 404232, 'created_utc': 1761046512.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.256481Z","level":"info","event":"error?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.256760Z","level":"info","event":"error 2?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.257000Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f42ec9f1c70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I recently joined my company, and they currently run dbt jobs using AWS Step Functions and a Fargate task that executes the project, and so on.\\n\\nHowever, I’m not sure if this is the best approach to orchestrate dbt jobs. Another important point is that the company manages most workflows through events following a DDD (Domain-Driven Design) pattern.\\n\\nRight now, there’s a case where a process depends on two different Step Functions before triggering another process.\\nThe challenge is that these Step Functions run at different times and don’t depend on each other.\\nAdditionally, in the future, there might be other processes that depend on those same Step Functions, but not necessarily on this one\\n\\nIn my opinion, Airflow doesn’t fit well here.\\n\\nWhat do you think would be a better way to manage these processes? Would it make sense to build something more custom for these types of cases', 'author_fullname': 't2_a7t1kvd5d', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What is the best way to orchestrate dbt job in aws', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1ocz7qu', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1761108844.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>I recently joined my company, and they currently run dbt jobs using AWS Step Functions and a Fargate task that executes the project, and so on.</p>\\n\\n<p>However, I’m not sure if this is the best approach to orchestrate dbt jobs. Another important point is that the company manages most workflows through events following a DDD (Domain-Driven Design) pattern.</p>\\n\\n<p>Right now, there’s a case where a process depends on two different Step Functions before triggering another process.\\nThe challenge is that these Step Functions run at different times and don’t depend on each other.\\nAdditionally, in the future, there might be other processes that depend on those same Step Functions, but not necessarily on this one</p>\\n\\n<p>In my opinion, Airflow doesn’t fit well here.</p>\\n\\n<p>What do you think would be a better way to manage these processes? Would it make sense to build something more custom for these types of cases</p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ocz7qu', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='jonathanrodrigr12'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ocz7qu/what_is_the_best_way_to_orchestrate_dbt_job_in_aws/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ocz7qu/what_is_the_best_way_to_orchestrate_dbt_job_in_aws/', 'subreddit_subscribers': 404232, 'created_utc': 1761108844.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.257274Z","level":"info","event":"error?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.257424Z","level":"info","event":"error 2?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.257620Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f42ec9f1c70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi all,\\n\\nI have some room booking data I need to do some time-related calculations with using Power Bi.\\n\\n1st table has room bookings data with room name, meeting start date time, meeting end date time, snapshot\\\\_date, etc.\\n\\nAs part of my ETL I am already building the snapshot\\\\_date rows based on the meeting start date time and meeting end date time.\\n\\n2nd table has room occupancy data which has room name, start date time, stop date time and usage which are in hour buckets.\\n\\nI have a dim date table connected to snapshot\\\\_date in the room bookings table and start date time in the room occupancy table.\\n\\nQuestion is do I need to have my room bookings data at the same time granularity (hourly) as the room occupancy data to make life easier with time calculations moving forward.\\n\\nCheers', 'author_fullname': 't2_1gaq3nkgru', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Date time granularity', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1oco9fs', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1761078928.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Hi all,</p>\\n\\n<p>I have some room booking data I need to do some time-related calculations with using Power Bi.</p>\\n\\n<p>1st table has room bookings data with room name, meeting start date time, meeting end date time, snapshot_date, etc.</p>\\n\\n<p>As part of my ETL I am already building the snapshot_date rows based on the meeting start date time and meeting end date time.</p>\\n\\n<p>2nd table has room occupancy data which has room name, start date time, stop date time and usage which are in hour buckets.</p>\\n\\n<p>I have a dim date table connected to snapshot_date in the room bookings table and start date time in the room occupancy table.</p>\\n\\n<p>Question is do I need to have my room bookings data at the same time granularity (hourly) as the room occupancy data to make life easier with time calculations moving forward.</p>\\n\\n<p>Cheers</p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1oco9fs', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ImFizzyGoodNice'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1oco9fs/date_time_granularity/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1oco9fs/date_time_granularity/', 'subreddit_subscribers': 404232, 'created_utc': 1761078928.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.257855Z","level":"info","event":"error?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.258000Z","level":"info","event":"error 2?","logger":"task.stdout"}
{"timestamp":"2025-10-22T06:02:44.258132Z","level":"info","event":"{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f42ec9f1c70>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': \"Hey guys, Cannot determine primary keys in raw data as no column is unique and concatenation of columns too don’t provide uniqueness even if I go by business logic and say these columns are pk I don’t get uniqueness, I get many duplicate rows, any idea on how to approach this? I can’t just remove those duplicates\\n\\nEDIT - I checked each column for uniqueness and concatenation of columns and checked their uniqueness by using distinct but nothing unique\\nI got duplicates and then I hashed all the columns together and removed the duplicate hashed columns and now I'm only hashing ID columns as other columns can like time and date can be changed and got some unique combo of columns that can be pk, I hope this approach is good guys \", 'author_fullname': 't2_1aagkt1u74', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Cannot determine primary keys in raw data as no column is unique and concatenation of columns too don’t provide uniqueness', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1oc6sq1', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.53, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1761052338.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1761032804.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Hey guys, Cannot determine primary keys in raw data as no column is unique and concatenation of columns too don’t provide uniqueness even if I go by business logic and say these columns are pk I don’t get uniqueness, I get many duplicate rows, any idea on how to approach this? I can’t just remove those duplicates</p>\\n\\n<p>EDIT - I checked each column for uniqueness and concatenation of columns and checked their uniqueness by using distinct but nothing unique\\nI got duplicates and then I hashed all the columns together and removed the duplicate hashed columns and now I&#39;m only hashing ID columns as other columns can like time and date can be changed and got some unique combo of columns that can be pk, I hope this approach is good guys </p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1oc6sq1', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Pleasant-Insect136'), 'discussion_type': None, 'num_comments': 53, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1oc6sq1/cannot_determine_primary_keys_in_raw_data_as_no/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1oc6sq1/cannot_determine_primary_keys_in_raw_data_as_no/', 'subreddit_subscribers': 404232, 'created_utc': 1761032804.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}","logger":"task.stdout"}
