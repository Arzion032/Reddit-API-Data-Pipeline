id,title,selftext,score,num_comments,author,subreddit_subscribers,created_utc,url,upvote_ratio,over_18,edited,spoiler,stickied
1occxjl,Developing with production data: who and how?,"Classic story: you should not work directly in prod, but apply the best devops practices, develop data pipelines in a development environment, debug, deploy in pre-prod, test, then deploy in production.

What about data analysts? data scientists? statisticians? ad-hoc reports?

Most data books focus on the data engineering lifecycle, sometimes they talk about the ""Analytics sandbox"", but they rarely address heads-on the personas doing analytics work in production. Modern data platform allow the decoupling of compute and data, enabling workload isolation to allow users read-only access to production data without affecting production workloads. Other teams perform data replication from production to lower environments. There's also the ""blue-green development architecture"", with two systems with production data.

How are you dealing with users requesting production data?",23,30,aburkh,404275,2025-10-21 13:26:54,https://www.reddit.com/r/dataengineering/comments/1occxjl/developing_with_production_data_who_and_how/,0.9,False,False,False,False
1ocst3a,Need advice choosing between Data engineer vs Sr Data analyst,"Hey all I could really use some career advice from this community.

I was fortunate to land 2 offers in this market, but now I’m struggling to make the right long term decision.

I’m finishing my Master’s in Data Science next semester. I interned last summer at a big company and then started working in my first FT data role as a data analyst at a small company (I’m about 6 months in). My goal is to eventually move into Data Science/ML maybe ML engineer and end up in big tech.

Option A: Data Engineer I
* Industry: Finance. This one pays $15k more. I’ll be working with a smaller team and I’d be the main technical person on the team. So no strong mentorship and I’ll have the pressure to “figure it out” on my own. 

Option B: Senior Data Analyst
* Industry: retail at a large org.

I’m nervous about being the only engineer on a team this early in my career…But I’m also worried about not being technical enough as a data analyst and not being technical. 

What would you do in my shoes?
Go hard into engineering now and level up fast even if it’s stressful without much support? Or take the analyst role at a big company, build brand and transition later?

Would appreciate any advice from people who’ve been on either path.
",14,18,Agile_Yak3819,404275,2025-10-21 23:41:05,https://www.reddit.com/r/dataengineering/comments/1ocst3a/need_advice_choosing_between_data_engineer_vs_sr/,0.82,False,False,False,False
1ocie74,Our 7 Snowflake query optimization tips and why they work,Hope y'all find it useful!,10,1,hornyforsavings,404275,2025-10-21 16:58:09,https://blog.greybeam.ai/snowflake-query-optimization/,0.86,False,False,False,False
1od40jc,PSA: Coordinated astroturfing campaign using LLM–driven bots to promote or manipulate SEO and public perception of several software vendors,"Patterns of possible automated bot activity promoting several vendors across r/dataengineering and broader Reddit have been detected.

Easy way to find dozens of bot accounts:  Find one shilling a bunch of tools then search these tools together.

Here's an [example query](https://www.reddit.com/search/?q=airbyte+dreamfactory&type=comments&sort=new) or[ this one](https://www.reddit.com/search/?q=airbyte+windsor&type=comments&sort=new) which find **dozens of bot users and hundreds of comments**. When pasting these comments to an LLM it will immediately identify patterns and highlight which vendors are being shilled with what tactic.

Community: **stay alert and report suspected bots. Tell your vendor if on the list that their tactics are backfiring.** When buying, consider vendor ethics, not just product features.

# Consequences exist! All it takes some pissed off reports.

Luckily astroturfing is illegal in all of the countries where these vendors are based.[ ](https://www.bloomberg.com/news/articles/2013-09-25/operation-clean-turf-and-the-war-on-fake-yelp-reviews)

[Here's what happened in 2013 to vendors with deceptive practise in sting operation ""clean turf"".](https://www.bloomberg.com/news/articles/2013-09-25/operation-clean-turf-and-the-war-on-fake-yelp-reviews) Founders and their CEOS were publicly named and shamed in major news outlets, like The Guardian, for personally orchestrating the fraud. Individuals were personally fined and forced to sign legally binding ""assurance of discontinuance"", in some cases prohibiting them from starting companies again.

For the 19 companies, the founders/owners were forced to personally pay fines ranging from $2,500 to just under $100,000 and sign an ""Assurance of Discontinuance,"" legally binding them to stop astroturfing.

# Reddit context

A [Reddit ban on AI bot research](https://www.reddit.com/r/Switzerland/comments/1kczxvw/reddit_bans_researchers_who_used_ai_bots_to/) shows how seriously this is taken. If that's ""a highly unethical experiment"" then **doing it for money instead of science is so much worse.**",9,0,EntireBurner2780,404275,2025-10-22 09:55:35,https://www.reddit.com/r/dataengineering/comments/1od40jc/psa_coordinated_astroturfing_campaign_using/,0.91,False,1761133167.0,False,False
1od2x2k,Parquet vs. Open Table Formats: Worth the Metadata Overhead?,"I recently ran into all sorts of pain working directly with raw Parquet files for an analytics project  broken schemas, partial writes, and painfully slow scans.   
That experience made me realize something simple: Parquet is *just* a storage format. It’s great at compression and column pruning, but that’s where it ends. No ACID guarantees, no safe schema evolution, no time travel, and a whole lot of chaos when multiple jobs touch the same data.

Then I explored open table formats like Apache Iceberg, Delta Lake, and Hudi  and it was like adding a missing layer of order on top  impressive is what they are bringing in 

* ACID transaction**s** through atomic metadata commits
* Schema evolution without having to rewrite everything
* for easy rollbacks and historical analysis we have Time travel 
* you can scan millions of files in milliseconds by Manifest indexing another cool thing 
* not to forget the hidden partitions 

In practice, these features made a *huge* difference  reliable BI queries running on the same data as streaming ETL jobs, painless GDPR-style deletes, and background compaction that keeps things tidy.

But it does make you think  is that extra metadata layer really worth the added complexity?  
 Or can clever workarounds and tooling keep raw Parquet setups running just fine at scale?

Wrote a blog on this that i am sharing here looking forward to your thoughts ",4,2,DevWithIt,404275,2025-10-22 08:46:11,https://olake.io/blog/iceberg-vs-parquet-table-format-vs-file-format,0.67,False,False,False,False
1ocqznw,Tools for automated migration away from Informatica,"Has anyone ever had any success using tools like DataBricks Lakebridge or Snowflake's SnowConvert to migrate Informatica powercenter ETL pipelines to another platform? I assume at best they ""kind of work sometimes for some things"" but am curious to hear anyone's actual experience with them in the wild.",6,2,BadKafkaPartitioning,404275,2025-10-21 22:22:39,https://www.reddit.com/r/dataengineering/comments/1ocqznw/tools_for_automated_migration_away_from/,1.0,False,False,False,False
1ocz7qu,What is the best way to orchestrate dbt job in aws,"I recently joined my company, and they currently run dbt jobs using AWS Step Functions and a Fargate task that executes the project, and so on.

However, I’m not sure if this is the best approach to orchestrate dbt jobs. Another important point is that the company manages most workflows through events following a DDD (Domain-Driven Design) pattern.

Right now, there’s a case where a process depends on two different Step Functions before triggering another process.
The challenge is that these Step Functions run at different times and don’t depend on each other.
Additionally, in the future, there might be other processes that depend on those same Step Functions, but not necessarily on this one

In my opinion, Airflow doesn’t fit well here.

What do you think would be a better way to manage these processes? Would it make sense to build something more custom for these types of cases",5,7,jonathanrodrigr12,404275,2025-10-22 04:54:04,https://www.reddit.com/r/dataengineering/comments/1ocz7qu/what_is_the_best_way_to_orchestrate_dbt_job_in_aws/,0.86,False,False,False,False
1od55tg,What's the best database IDE for Mac?,Because SQL Server is not possible to install and maybe you have other DDBB in Amazon or Oracle,3,14,Irachar,404275,2025-10-22 11:02:17,https://www.reddit.com/r/dataengineering/comments/1od55tg/whats_the_best_database_ide_for_mac/,1.0,False,False,False,False
1od2c18,"Data Modeling for the Agentic Era: Semantics, Speed, and Stewardship",,2,0,sspaeti,404275,2025-10-22 08:07:45,https://www.rilldata.com/blog/data-modeling-for-the-agentic-era-semantics-speed-and-stewardship,0.76,False,False,False,False
1od0cqr,Anyone experienced with jOOQ as SQL transpiler?,Does anyone have experience with jOOQ (https://github.com/jOOQ/jOOQ) as a transpiler between two different SQL dialects? We are searching for options in Java to run queries from other dialects on Exasol without the users having to rewrite them.,2,0,exagolo,404275,2025-10-22 06:01:32,https://www.reddit.com/r/dataengineering/comments/1od0cqr/anyone_experienced_with_jooq_as_sql_transpiler/,0.67,False,False,False,False
1ocy398,The Death of Thread Per Core,,2,0,sionescu,404275,2025-10-22 03:53:22,https://buttondown.com/jaffray/archive/the-death-of-thread-per-core/,0.67,False,False,False,False
1ocksmo,Thoughts on Using Synthetic Tabular data for DE projects ?,"Thoughts on using Synthetic Data for Projects ?

I'm currently a DB Specialist with 3 YOE learning Spark, DBT, Python, Airflow and AWS to switch to DE roles.

I’d love some feedback on a portfolio project I’m working on. It’s basically a modernized spin on the kind of work I do at my job, a Transaction Data Platform with a multi-step ETL pipeline.

Quick overview of setup:

DB structure:

Dimensions = Bank -> Account -> Routing

Fact = Transactions -> Transaction\_Steps

History = Hist_Transactions -> Hist_Transaction_Steps (identical to fact tables, just one extra column) 

I mocked up 3 regions -> 3 banks per region -> 3 accounts per bank -> 702 unique directional routings.

A Python script first assigns following parameters to each routing:

type (High Intensity/Frequency/Normal)

country\_code, region, cross\_border

base\_freq, base\_amount, base\_latency, base\_success

volatility vars (freq/amount/latency/success)

Then the synthesizer script uses above paramters to spit out 85k-135k records per day, and 5x times Transaction\_Steps 

Anomaly engine randomly spikes volatility (50–250x) \~5 times a week for a random routing, the aim is (hopefully) the pipeline will detect the anomalies.

Pipeline workflow:

Batch runs daily (simulating off business hours migration).

Every day data older than 1 month in live table is moved to history tables (partitioned by day and OLTP compressed) 

Then the partitions older than a month in history tables are exported to Parquet (maybe I'll create a Data lake or something) cold storage and stored. 

The current day's transactions are transformed through DBT, to generate 12 marts, helping in anomaly detection and system monitoring 

A Great Expectation + Python layer takes care of data quality and Anomaly detection

Finally for visualization and ease of discussion I'm generating a streamlit dashboard from above 12 marts.

Main concerns/questions:

1. Since this is just inspired by my current work (I didn’t use real table names/logic, just the concept), should I be worried about IP/overlap ?
2. I’ve done a barebones version of this in shell+SQL, so I personally know business and technical requirements and possible issues in this project, it feels really straightforward.  Do you think this is a solid enough project to showcase for DE roles at product-based-companies / fintechs (0–3 YOE range)?
3. Thoughts on using synthetic data? I’ve tried to make it noisy and realistic, but since I’ll always have control, I feel like I'm missing something critical that only shows up in real-world messy data?

Would love any outside perspective

This would ideally be the portfolio project, and there's one more planned using spark where I'm just cleaning and merging Spotify datasets from different types (CSV, json, sqlite, parquet etc) from Kaggle, it's just a practice project to showcase spark understanding. 

**TLDR:**  
Built a synthetic transaction pipeline (750k+ txns, 3.75M steps, anomaly injection, DBT marts, cold storage). Looking for feedback on:

* IP concerns (inspired by work but no copied code/keywords)
* Whether it’s a strong enough DE project for Product Based Companies and Fintech. 
* Pros/cons of using synthetic vs real-world messy data",2,3,Markymark285,404275,2025-10-21 18:26:56,https://www.reddit.com/r/dataengineering/comments/1ocksmo/thoughts_on_using_synthetic_tabular_data_for_de/,0.67,False,False,False,False
1ocery5,Anyone else use AWS Redshift Zero-ETL in US-EAST-1?,"This is a service that basically puts a read replica of an RDS streaming into your Redshift data warehouse. 

We have this set up in our environment and it runs many critical systems. After the nightmares of yesterday I checked this morning after getting some complaints from unhappy users about stale data and our ZETL integrations appear to have disappeared entirely. I can see the data and it appears to have stopped updating coincident with yesterday's outage. Looks like I'll have to completely remake these. This is pretty irritating because I can't find any information anywhere from AWS about the outage having *deleted* this infrastructure.",1,1,bingbongbangchang,404275,2025-10-21 14:40:29,https://www.reddit.com/r/dataengineering/comments/1ocery5/anyone_else_use_aws_redshift_zeroetl_in_useast1/,0.56,False,False,False,False
1occmld,DBT unit tests on Redshift,"Hello!

I'm trying to implement unit tests for the DBT models used by my team, so we have more trust on those models and their logic. However I'm getting stuck when the model contains a SUPER-typed column for JSON data.

If I write the JSON object inside a string in the YAML file of the test, then DBT expects unquoted JSON. If I remove the quotes around the JSOn object, then I get a syntax error on the YAML file. I also tried writing the JSON object as a YAML object with indent but it fails too. 

What should I do ?",2,5,Adrien0623,404275,2025-10-21 13:14:11,https://www.reddit.com/r/dataengineering/comments/1occmld/dbt_unit_tests_on_redshift/,0.63,False,False,False,False
1od0cdl,What do you think it would happen to dbt certificates since it was merged with Fivetran?,"A bit of context, I am coming from non tech education so I am always looking for ways to learn more about data engineering. 

I was thinking to prepare for the dbt certificates but since the news came out. I am observing. 

What do you think?",1,2,Ragnareuk,404275,2025-10-22 06:00:57,https://www.reddit.com/r/dataengineering/comments/1od0cdl/what_do_you_think_it_would_happen_to_dbt/,0.6,False,False,False,False
1ocvodq,Data Streaming Delivery Semantics,"https://preview.redd.it/h4k1298ikkwf1.png?width=1744&format=png&auto=webp&s=ebf666f0ad1f466edc7e8802e4087f65c35dfd4c

[https://codepen.io/gangtao/full/raxdOOK](https://codepen.io/gangtao/full/raxdOOK) ",1,0,gangtao,404275,2025-10-22 01:54:02,https://www.reddit.com/r/dataengineering/comments/1ocvodq/data_streaming_delivery_semantics/,0.67,False,False,False,False
1oce5db,Schema Evolution in GCP,"Hi, i have experience with ELT pipelines on GCP. One that loads data from Postgres tables data into BigQuery and another one that loads CSV files data from SFTP server to BigQuery. 

* Postgres to BQ:

Source -> BQ Bronze layer 
Dataflow (custom template for multi table ingestion in single job)

BQ Bronze-> BQ Silver Layer -> BQ Gold 

————————————————————

* SFTP to BQ:

SFTP -> GCS
Airflow (Composer) SFTP to GCS operator

GCS -> BQ Bronze
GCS to BQ airflow operator

BQ Bronze layer -> BQ Silver Layer -> BQ Gold 

—————————————————————

BQ bronze to silver in both cases is handled using a stored procedure that handles upsert of Bronze data (cleaned data) to Silver. Dataform for Silver to Gold layer.

Now, could you please explain how below could be handled in both cases?:

1. Schema Evolution 
2. Handling both incremental or Full load as per requirement using same pipeline (if possible)
3. Handling Large Volumes of data (TB) 
4. Frameworks/tools/methods for Data Quality checks and data validations


Thank you in advance! 


",1,0,FeeOk6875,404275,2025-10-21 14:15:38,https://www.reddit.com/r/dataengineering/comments/1oce5db/schema_evolution_in_gcp/,0.57,False,False,False,False
1oco9fs,Date time granularity,"Hi all,

I have some room booking data I need to do some time-related calculations with using Power Bi.

1st table has room bookings data with room name, meeting start date time, meeting end date time, snapshot\_date, etc.

As part of my ETL I am already building the snapshot\_date rows based on the meeting start date time and meeting end date time.

2nd table has room occupancy data which has room name, start date time, stop date time and usage which are in hour buckets.

I have a dim date table connected to snapshot\_date in the room bookings table and start date time in the room occupancy table.

Question is do I need to have my room bookings data at the same time granularity (hourly) as the room occupancy data to make life easier with time calculations moving forward.

Cheers",0,2,ImFizzyGoodNice,404275,2025-10-21 20:35:28,https://www.reddit.com/r/dataengineering/comments/1oco9fs/date_time_granularity/,0.4,False,False,False,False
